{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer of Donald Trump's tweets\n",
    "### A project for the AI course Advanced Natural Language Processing\n",
    "_Rik Dijkstra, Abel de Wit, Max Knappe_\n",
    "\n",
    "Every piece of text fits in a specific time, place and scenario, conveys specific characteristics of the user of language and has a specific intent. If we denote the piece of text as `x` and the style of this text as `a`. Text Style Transfer (TST) aims to produce text `x` of a desired attribute value `a`, given the existing text `x'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./nltk_data/...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./nltk_data/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to ./nltk_data/...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     ./nltk_data/...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', download_dir='./nltk_data/')\n",
    "nltk.download('stopwords', download_dir='./nltk_data/')\n",
    "nltk.download('wordnet', download_dir='./nltk_data/')\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='./nltk_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/realdonaldtrump.csv')\n",
    "df2 = pd.read_csv('./data/trumptweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698308935</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701461182</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1737479987</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1741160716</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1773561338</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               link  \\\n",
       "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 13:54:25   \n",
       "1  Donald Trump will be appearing on The View tom...  2009-05-04 20:00:10   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 08:38:08   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 15:40:15   \n",
       "4  \"My persona will never be that of a wallflower...  2009-05-12 09:07:28   \n",
       "\n",
       "   retweets  favorites mentions hashtags  \n",
       "0       510        917      NaN      NaN  \n",
       "1        34        267      NaN      NaN  \n",
       "2        13         19      NaN      NaN  \n",
       "3        11         26      NaN      NaN  \n",
       "4      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698308935</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 20:54:25</td>\n",
       "      <td>500</td>\n",
       "      <td>868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701461182</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-05 03:00:10</td>\n",
       "      <td>33</td>\n",
       "      <td>273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1737479987</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 15:38:08</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1741160716</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 22:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1773561338</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 16:07:28</td>\n",
       "      <td>1399</td>\n",
       "      <td>1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               link  \\\n",
       "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 20:54:25   \n",
       "1  Donald Trump will be appearing on The View tom...  2009-05-05 03:00:10   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 15:38:08   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 22:40:15   \n",
       "4  \"My persona will never be that of a wallflower...  2009-05-12 16:07:28   \n",
       "\n",
       "   retweets  favorites mentions hashtags  geo  \n",
       "0       500        868      NaN      NaN  NaN  \n",
       "1        33        273      NaN      NaN  NaN  \n",
       "2        12         18      NaN      NaN  NaN  \n",
       "3        11         24      NaN      NaN  NaN  \n",
       "4      1399       1965      NaN      NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing duplicates**\n",
    "\n",
    "As we can already see in the first ten entries of both datasets, there are some duplicate tweets. Let's combine the two datasets and remove the duplicates based on the 'content' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two datasets together were 84474 tweets long, of which 40947 were duplicates,\n",
      "this leaves us with 43527 tweets\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df1, df2])\n",
    "len_before = len(df)\n",
    "df = df.drop_duplicates(subset=['content'], ignore_index=True)\n",
    "len_after = len(df)\n",
    "print(\"The two datasets together were {} tweets long, of which {} were duplicates,\\nthis leaves us with {} tweets\".format(len_before, (len_before - len_after), len_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Now that we have a set of unique tweets from Trump, we need to pre-process the data such that hyperlinks, named entities and other attributes that are not part of The Donald's style of writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "link             0\n",
       "content          0\n",
       "date             0\n",
       "retweets         0\n",
       "favorites        0\n",
       "mentions     22842\n",
       "hashtags     37870\n",
       "geo          43527\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the column that we want to work with ('content') has no empty fields, so we don't have to remove any of our entries\n",
    "\n",
    "Next up is our pre-processing where we remove text that is not useful for our model such as hyperlinks, numbers and dates, and decapitalization of our text. After that, we tokenize the sentences so we have a list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>[be, sure, to, tune, in, and, watch, donald, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>[donald, trump, will, be, appearing, on, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>[donald, trump, reads, top, ten, financial, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>[new, blog, post, celebrity, apprentice, final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>[my, persona, will, never, be, that, of, a, wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...   \n",
       "1  Donald Trump will be appearing on The View tom...   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...   \n",
       "4  \"My persona will never be that of a wallflower...   \n",
       "\n",
       "                                       clean_content  \n",
       "0  [be, sure, to, tune, in, and, watch, donald, t...  \n",
       "1  [donald, trump, will, be, appearing, on, the, ...  \n",
       "2  [donald, trump, reads, top, ten, financial, ti...  \n",
       "3  [new, blog, post, celebrity, apprentice, final...  \n",
       "4  [my, persona, will, never, be, that, of, a, wa...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)              # Remove Hyperlinks\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)           # Remove non-alphanumeric\n",
    "    text = str(text).lower()                         # Change all to lowercase\n",
    "    text = re.sub(r'(donald j?.? trump)', '', text)  # Remove all his name occurrences\n",
    "    text = word_tokenize(text)                       # Tokenize sentence\n",
    "    return text\n",
    "\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_text)\n",
    "df.head(5)[['content', 'clean_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal, and lemmatization\n",
    "**I am not sure if this should be done, as stopwords might be part of Trump's style**\n",
    "\n",
    "Stop words are too common in a language and teach us nothing about the meaning or style of a text. Hence we remove them. Some words can have inflectional forms, such as `saw` and `see`. And since we want to learn our model that these words are the same as well, we apply lemmatization which converts each inflectional form to their base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>[sure, tune, watch, donald, trump, late, night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>[donald, trump, appear, view, tomorrow, morn, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>[donald, trump, read, top, ten, financi, tip, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>[new, blog, post, celebr, apprentic, final, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>[persona, never, wallflow, rather, build, wall...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...   \n",
       "1  Donald Trump will be appearing on The View tom...   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...   \n",
       "4  \"My persona will never be that of a wallflower...   \n",
       "\n",
       "                                       clean_content  \n",
       "0  [sure, tune, watch, donald, trump, late, night...  \n",
       "1  [donald, trump, appear, view, tomorrow, morn, ...  \n",
       "2  [donald, trump, read, top, ten, financi, tip, ...  \n",
       "3  [new, blog, post, celebr, apprentic, final, le...  \n",
       "4  [persona, never, wallflow, rather, build, wall...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean_stop_lemma(token):\n",
    "    text = [item for item in token if item not in stop_words]  # Remove stopwords\n",
    "    text = [stemmer.stem(i) for i in text]                     # Stem words with inflections\n",
    "    text = [lemma.lemmatize(word=w, pos='v') for w in text]    # Lemmatize words with inflections\n",
    "    return text\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(clean_stop_lemma)\n",
    "df.head(5)[['content', 'clean_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "Now we generate the word embedding dictionaries where we have `word2index`, `index2word`, and `word2count`. This allows us to transform the text to numbers and back, and gives us an overview of the occurrances of words in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our trump_tweets consists of 32552 unique words.\n"
     ]
    }
   ],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word_to_idx = {'<s>': 0, '</s>': 1}\n",
    "        self.idx_to_word = {0: '<s>', 1: '</s>'}\n",
    "        self.word_to_count = {}\n",
    "        \n",
    "    def generate_dict(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                if word not in self.word_to_idx:\n",
    "                    self.word_to_idx[word] = len(self.word_to_idx)\n",
    "                    self.word_to_count[word] = 1\n",
    "                    self.idx_to_word[len(self.word_to_idx)] = word\n",
    "                else:\n",
    "                    self.word_to_count[word] += 1\n",
    "\n",
    "trump_vocab = Vocabulary('trump_tweets')\n",
    "trump_vocab.generate_dict(list(df['clean_content']))\n",
    "print(\"Our {} consists of {} unique words.\".format(trump_vocab.name, len(trump_vocab.word_to_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Normal' tweets\n",
    "The next database that we're going to use is a collection of tweets of the 20 most popular twitter database. We will apply the same preprocessing to this database, so we can train a classifier to recognize which tweets belong to trump, and which don't. With this classifier as our metric, we can then create a Sequence to Sequence model that will train to deceive our classifier in creating realistic Trump tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = pd.read_csv('./data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                  0\n",
       "content                 0\n",
       "country             52506\n",
       "date_time               0\n",
       "id                      0\n",
       "language                0\n",
       "latitude            52541\n",
       "longitude           52541\n",
       "number_of_likes         0\n",
       "number_of_shares        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df['clean_content'] = normal_df['content'].apply(clean_text)\n",
    "normal_df['clean_content'] = normal_df['clean_content'].apply(clean_stop_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our normal_tweets consists of 34278 unique words.\n"
     ]
    }
   ],
   "source": [
    "normal_vocab = Vocabulary('normal_tweets')\n",
    "normal_vocab.generate_dict(list(normal_df['clean_content']))\n",
    "print(\"Our {} consists of {} unique words.\".format(normal_vocab.name, len(normal_vocab.word_to_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
