{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer of Donald Trump's tweets\n",
    "### A project for the AI course Advanced Natural Language Processing\n",
    "_Rik Dijkstra, Abel de Wit, Max Knappe_\n",
    "\n",
    "Every piece of text fits in a specific time, place and scenario, conveys specific characteristics of the user of language and has a specific intent. If we denote the piece of text as `x` and the style of this text as `a`. Text Style Transfer (TST) aims to produce text `x` of a desired attribute value `a`, given the existing text `x'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', download_dir='./nltk_data/')\n",
    "nltk.download('stopwords', download_dir='./nltk_data/')\n",
    "nltk.download('wordnet', download_dir='./nltk_data/')\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='./nltk_data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/realdonaldtrump.csv')\n",
    "df2 = pd.read_csv('./data/trumptweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing duplicates**\n",
    "\n",
    "As we can already see in the first ten entries of both datasets, there are some duplicate tweets. Let's combine the two datasets and remove the duplicates based on the 'content' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2])\n",
    "len_before = len(df)\n",
    "df = df.drop_duplicates(subset=['content'], ignore_index=True)\n",
    "len_after = len(df)\n",
    "print(\"The two datasets together were {} tweets long, of which {} were duplicates,\\nthis leaves us with {} tweets\".format(len_before, (len_before - len_after), len_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Now that we have a set of unique tweets from Trump, we need to pre-process the data such that hyperlinks, named entities and other attributes that are not part of The Donald's style of writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the column that we want to work with ('content') has no empty fields, so we don't have to remove any of our entries\n",
    "\n",
    "Next up is our pre-processing where we remove text that is not useful for our model such as hyperlinks, numbers and dates, and decapitalization of our text. After that, we tokenize the sentences so we have a list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)              # Remove Hyperlinks\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)           # Remove non-alphanumeric\n",
    "    text = str(text).lower()                         # Change all to lowercase\n",
    "    text = re.sub(r'(donald j?.? trump)', '', text)  # Remove all his name occurrences\n",
    "    text = word_tokenize(text)                       # Tokenize sentence\n",
    "    return text\n",
    "\n",
    "\n",
    "df['clean_content'] = df['content'].apply(clean_text)\n",
    "df.head(5)[['content', 'clean_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal, and lemmatization\n",
    "**I am not sure if this should be done, as stopwords might be part of Trump's style**\n",
    "\n",
    "Stop words are too common in a language and teach us nothing about the meaning or style of a text. Hence we remove them. Some words can have inflectional forms, such as `saw` and `see`. And since we want to learn our model that these words are the same as well, we apply lemmatization which converts each inflectional form to their base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean_stop_lemma(token):\n",
    "    text = [item for item in token if item not in stop_words]  # Remove stopwords\n",
    "    text = [stemmer.stem(i) for i in text]                     # Stem words with inflections\n",
    "    text = [lemma.lemmatize(word=w, pos='v') for w in text]    # Lemmatize words with inflections\n",
    "    return text\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(clean_stop_lemma)\n",
    "df.head(5)[['content', 'clean_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "Now we generate the word embedding dictionaries where we have `word2index`, `index2word`, and `word2count`. This allows us to transform the text to numbers and back, and gives us an overview of the occurrances of words in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word_to_idx = {'<s>': 0, '</s>': 1}\n",
    "        self.idx_to_word = {0: '<s>', 1: '</s>'}\n",
    "        self.word_to_count = {}\n",
    "        self.all_words = []\n",
    "        \n",
    "    def generate_dict(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                if word not in self.word_to_idx:\n",
    "                    self.word_to_idx[word] = len(self.word_to_idx)\n",
    "                    self.word_to_count[word] = 1\n",
    "                    self.idx_to_word[len(self.word_to_idx)] = word\n",
    "                else:\n",
    "                    self.word_to_count[word] += 1\n",
    "        self.all_words = list(self.word_to_count.keys())\n",
    "\n",
    "trump_vocab = Vocabulary('trump_tweets')\n",
    "trump_vocab.generate_dict(list(df['clean_content']))\n",
    "print(\"Our {} consists of {} unique words.\".format(trump_vocab.name, len(trump_vocab.word_to_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Normal' tweets\n",
    "The next database that we're going to use is a collection of tweets of the 20 most popular twitter database. We will apply the same preprocessing to this database, so we can train a classifier to recognize which tweets belong to trump, and which don't. With this classifier as our metric, we can then create a Sequence to Sequence model that will train to deceive our classifier in creating realistic Trump tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = pd.read_csv('./data/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df['clean_content'] = normal_df['content'].apply(clean_text)\n",
    "normal_df['clean_content'] = normal_df['clean_content'].apply(clean_stop_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_vocab = Vocabulary('normal_tweets')\n",
    "normal_vocab.generate_dict(list(normal_df['clean_content']))\n",
    "print(\"Our {} consists of {} unique words.\".format(normal_vocab.name, len(normal_vocab.word_to_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trump tweet classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trump'] = True\n",
    "normal_df['trump'] = False\n",
    "\n",
    "dfs = [df[['clean_content', 'trump']], normal_df[['clean_content', 'trump']]]\n",
    "\n",
    "cdf = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab = Vocabulary('all_data')\n",
    "full_vocab.generate_dict(list(cdf['clean_content']))\n",
    "print(\"Our {} consists of {} unique words.\".format(full_vocab.name, len(full_vocab.word_to_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexEmpty = cdf[cdf['clean_content'].map(lambda d: len(d)) == 0].index\n",
    "print(\"After cleaning there are {} empty tweets, let's drop those\".format(len(indexEmpty)))\n",
    "cdf.drop(indexEmpty, inplace=True)\n",
    "\n",
    "print(\"Now we have {} Trump tweets, and {} normal tweets\".format(len(cdf[cdf['trump'] == True]), len(cdf[cdf['trump'] == False])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
