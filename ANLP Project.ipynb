{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer of Donald Trump's tweets\n",
    "### A project for the AI course Advanced Natural Language Processing\n",
    "_Rik Dijkstra, Abel de Wit, Max Knappe_\n",
    "\n",
    "Every piece of text fits in a specific time, place and scenario, conveys specific characteristics of the user of language and has a specific intent. If we denote the piece of text as `x` and the style of this text as `a`. Text Style Transfer (TST) aims to produce text `x` of a desired attribute value `a`, given the existing text `x'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/abeldewit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abeldewit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abeldewit/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/abeldewit/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading in the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('./data/realdonaldtrump.csv')\n",
    "df2 = pd.read_csv('./data/trumptweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698308935</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 13:54:25</td>\n",
       "      <td>510</td>\n",
       "      <td>917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701461182</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-04 20:00:10</td>\n",
       "      <td>34</td>\n",
       "      <td>267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1737479987</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 08:38:08</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1741160716</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 15:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1773561338</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 09:07:28</td>\n",
       "      <td>1375</td>\n",
       "      <td>1945</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               link  \\\n",
       "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 13:54:25   \n",
       "1  Donald Trump will be appearing on The View tom...  2009-05-04 20:00:10   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 08:38:08   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 15:40:15   \n",
       "4  \"My persona will never be that of a wallflower...  2009-05-12 09:07:28   \n",
       "\n",
       "   retweets  favorites mentions hashtags  \n",
       "0       510        917      NaN      NaN  \n",
       "1        34        267      NaN      NaN  \n",
       "2        13         19      NaN      NaN  \n",
       "3        11         26      NaN      NaN  \n",
       "4      1375       1945      NaN      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698308935</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/169...</td>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>2009-05-04 20:54:25</td>\n",
       "      <td>500</td>\n",
       "      <td>868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1701461182</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/170...</td>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>2009-05-05 03:00:10</td>\n",
       "      <td>33</td>\n",
       "      <td>273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1737479987</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/173...</td>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>2009-05-08 15:38:08</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1741160716</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/174...</td>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>2009-05-08 22:40:15</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1773561338</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/177...</td>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>2009-05-12 16:07:28</td>\n",
       "      <td>1399</td>\n",
       "      <td>1965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               link  \\\n",
       "0  1698308935  https://twitter.com/realDonaldTrump/status/169...   \n",
       "1  1701461182  https://twitter.com/realDonaldTrump/status/170...   \n",
       "2  1737479987  https://twitter.com/realDonaldTrump/status/173...   \n",
       "3  1741160716  https://twitter.com/realDonaldTrump/status/174...   \n",
       "4  1773561338  https://twitter.com/realDonaldTrump/status/177...   \n",
       "\n",
       "                                             content                 date  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...  2009-05-04 20:54:25   \n",
       "1  Donald Trump will be appearing on The View tom...  2009-05-05 03:00:10   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...  2009-05-08 15:38:08   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...  2009-05-08 22:40:15   \n",
       "4  \"My persona will never be that of a wallflower...  2009-05-12 16:07:28   \n",
       "\n",
       "   retweets  favorites mentions hashtags  geo  \n",
       "0       500        868      NaN      NaN  NaN  \n",
       "1        33        273      NaN      NaN  NaN  \n",
       "2        12         18      NaN      NaN  NaN  \n",
       "3        11         24      NaN      NaN  NaN  \n",
       "4      1399       1965      NaN      NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing duplicates**\n",
    "\n",
    "As we can already see in the first ten entries of both datasets, there are some duplicate tweets. Let's combine the two datasets and remove the duplicates based on the 'content' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two datasets together were 84474 tweets long, of which 40947 were duplicates,\n",
      "this leaves us with 43527 tweets\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df1, df2])\n",
    "len_before = len(df)\n",
    "df = df.drop_duplicates(subset=['content'], ignore_index=True)\n",
    "len_after = len(df)\n",
    "print(\"The two datasets together were {} tweets long, of which {} were duplicates,\\nthis leaves us with {} tweets\".format(len_before, (len_before - len_after), len_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Now that we have a set of unique tweets from Trump, we need to pre-process the data such that hyperlinks, named entities and other attributes that are not part of The Donald's style of writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "link             0\n",
       "content          0\n",
       "date             0\n",
       "retweets         0\n",
       "favorites        0\n",
       "mentions     22842\n",
       "hashtags     37870\n",
       "geo          43527\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the column that we want to work with ('content') has no empty fields, so we don't have to remove any of our entries\n",
    "\n",
    "Next up is our pre-processing where we remove text that is not useful for our model such as hyperlinks, numbers and dates, and decapitalization of our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyper(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "def clean_non_alphanumeric(text):\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "def clean_lowercase(text):\n",
    "    return str(text).lower()\n",
    "\n",
    "df['clean_content'] = df['content'].apply(remove_hyper)\n",
    "df['clean_content'] = df['clean_content'].apply(clean_non_alphanumeric)\n",
    "df['clean_content'] = df['clean_content'].apply(clean_lowercase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we tokenize the text so that is becomes a list of all the tokens in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenization(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(clean_tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, the text has been tokenized and put in a new column in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Be sure to tune in and watch Donald Trump on L...</td>\n",
       "      <td>[be, sure, to, tune, in, and, watch, donald, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump will be appearing on The View tom...</td>\n",
       "      <td>[donald, trump, will, be, appearing, on, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donald Trump reads Top Ten Financial Tips on L...</td>\n",
       "      <td>[donald, trump, reads, top, ten, financial, ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Blog Post: Celebrity Apprentice Finale and...</td>\n",
       "      <td>[new, blog, post, celebrity, apprentice, final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"My persona will never be that of a wallflower...</td>\n",
       "      <td>[my, persona, will, never, be, that, of, a, wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Be sure to tune in and watch Donald Trump on L...   \n",
       "1  Donald Trump will be appearing on The View tom...   \n",
       "2  Donald Trump reads Top Ten Financial Tips on L...   \n",
       "3  New Blog Post: Celebrity Apprentice Finale and...   \n",
       "4  \"My persona will never be that of a wallflower...   \n",
       "\n",
       "                                       clean_content  \n",
       "0  [be, sure, to, tune, in, and, watch, donald, t...  \n",
       "1  [donald, trump, will, be, appearing, on, the, ...  \n",
       "2  [donald, trump, reads, top, ten, financial, ti...  \n",
       "3  [new, blog, post, celebrity, apprentice, final...  \n",
       "4  [my, persona, will, never, be, that, of, a, wa...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)[['content', 'clean_content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal, and lemmatization\n",
    "**I am not sure if this should be done, as stopwords might be part of Trump's style**\n",
    "\n",
    "Stop words are too common in a language and lean us nothing about the meaning or style of a text. Hence we remove them. Some words can have inflectional forms, such as `saw` and `see`. And since we want to learn our model that these words are the same as well, we apply lemmatization which converts each inflectional form to their base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_stopwords(token):\n",
    "    return [item for item in token if item not in stop_words]\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(clean_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "def clean_stem(token):\n",
    "    return [stemmer.stem(i) for i in token]\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(clean_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "def clean_lemma(token):\n",
    "    return [lemma.lemmatize(word=w, pos='v') for w in token]\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(clean_lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "Now we generate the word embedding dictionaries where we have `word2index`, `index2word`, and `word2count`. This allo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dict(corpus):\n",
    "    word_to_idx = {'<s>': 0, '</s>': 1}\n",
    "    idx_to_word = {0: '<s>', 1: '</s>'}\n",
    "    word_tp_count = {}\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        for word in sentence:\n",
    "            if word not in word_count.keys():\n",
    "                word_count[word] = 1\n",
    "            else:\n",
    "                word_count[word] += 1\n",
    "            \n",
    "            if word not in word_to_idx.keys():\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
